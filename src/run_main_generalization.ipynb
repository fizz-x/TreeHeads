{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1d21fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.merge import merge\n",
    "from rasterio.warp import reproject, Resampling\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "\n",
    "#import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import utils.basics as bsc \n",
    "import utils.plotting as pt\n",
    "import utils.processing as proc\n",
    "import utils.eval_pipe as eval\n",
    "\n",
    "import utils.model_loader as md\n",
    "import utils.data_loader as dt\n",
    "import utils.config_loader as cf\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6c4a41",
   "metadata": {},
   "source": [
    "## main run of experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2a002b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT EXPERIMENTAL CONFIG\n",
    "with open('../configs/experiments.yaml', 'r') as f:\n",
    "    experiments = yaml.safe_load(f)\n",
    "    experiment_names = list(experiments.keys())\n",
    "    experiment_names = experiment_names[1:8]  # Select the first 6 experiment names\n",
    "\n",
    "    \n",
    "with open('../configs/normparams.yaml', 'r') as f:\n",
    "    normparams = yaml.safe_load(f)\n",
    "joint_normparams = normparams['chm']['_111']\n",
    "#print(experiment_names)\n",
    "experiment_names\n",
    "combos = [\"110\",\"101\",\"011\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fca4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "global_config = md.global_config\n",
    "seed = global_config['seed'] \n",
    "run_id_base = \"251025_GEN_\" # customize as needed\n",
    "repetitions = range(5)\n",
    "combos = [\"110\",\"101\",\"011\"] #1 is training data, 0 test. logic is LSB: 001 -> SITE1 is training data. \n",
    "\n",
    "for i in repetitions: # Run 10 experiments with different seeds\n",
    "\n",
    "    for combo in combos:\n",
    "        #run_id = md.generate_run_id()\n",
    "        run_id = run_id_base + f\"_{combo}_{i}\"\n",
    "        seed = seed + i  # Different seed for each experiment\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "        for exp_name in experiment_names:\n",
    "            \n",
    "            sites, cfg = cf.get_config(exp_name)  \n",
    "            cfg.update(global_config)  # Ensure cfg has the latest global_config\n",
    "            cfg.update({\"combo\": combo})\n",
    "            #print(\"=== NEW EXPERIMENT ===\")\n",
    "            print(f\" --> Name: {exp_name}, Combo: {combo}, Run ID: {run_id}, Seed: {seed}\")\n",
    "\n",
    "            # Build dataset\n",
    "            X, Y, sitenums = dt.build_patched_dataset(cfg, sites, patch_size=32, nan_percent_allowed=20)\n",
    "            X_patch_train, Y_patch_train, X_test, Y_test = dt.build_patched_dataset_generalization(cfg, sites, combo,patch_size=32, nan_percent_allowed=20)\n",
    "            \n",
    "            # now split the train data into train (80%) and val (20%)\n",
    "            X_train, X_val, Y_train, Y_val = train_test_split(\n",
    "                X_patch_train, Y_patch_train, test_size=0.2, random_state=seed\n",
    "            )\n",
    "\n",
    "\n",
    "            # Syntax: X_train, X_val, X_test, y_train, y_val, y_test \n",
    "            train_dataset = md.S2CanopyHeightDataset(X_train, Y_train, cfg)\n",
    "            val_dataset = md.S2CanopyHeightDataset(X_val, Y_val, cfg)\n",
    "            test_dataset = md.S2CanopyHeightDataset(X_test, Y_test, cfg)\n",
    "\n",
    "            train_loader = md.DataLoader(\n",
    "                train_dataset,\n",
    "                batch_size=global_config['batch_size'],\n",
    "                shuffle=True,\n",
    "                num_workers=0\n",
    "            )\n",
    "            val_loader = md.DataLoader(val_dataset, batch_size=global_config['batch_size'])\n",
    "            test_loader = md.DataLoader(test_dataset, batch_size=global_config['batch_size'])\n",
    "            \n",
    "            # build model depending on in out channels, defined by the dataloaders\n",
    "            model = md.build_unet(in_channels=X_train[0].shape[0], out_channels=Y_train[0].shape[0],cfg=cfg)\n",
    "            #train model depending on config. \n",
    "            model, logs = md.train_model(model, train_loader, val_loader, cfg)\n",
    "\n",
    "            # evaluate model on val and test set, save results\n",
    "            # get normparams for CHMmmn\n",
    "            combo_key = f\"_{combo}\"\n",
    "            normparams_used = normparams['chm'][combo_key]\n",
    "\n",
    "            md.save_results(model, val_loader, test_loader, normparams_used, logs, cfg, run_id=run_id)\n",
    "\n",
    "            #print(\"=================\")\n",
    "\n",
    "        #print(\"DONE WITH COMBO \", combo)\n",
    "    print(\"DONE WITH ALL EXPERIMENTS for iteration:  \", i)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CHM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1d21fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.merge import merge\n",
    "from rasterio.warp import reproject, Resampling\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "\n",
    "#import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import utils.basics as bsc \n",
    "import utils.plotting as pt\n",
    "import utils.processing as proc\n",
    "import utils.eval_pipe as eval\n",
    "\n",
    "import utils.model_loader as md\n",
    "import utils.data_loader as dt\n",
    "import utils.config_loader as cf\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6c4a41",
   "metadata": {},
   "source": [
    "## main run of experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4f2a002b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT EXPERIMENTAL CONFIG\n",
    "with open('../configs/experiments.yaml', 'r') as f:\n",
    "    experiments = yaml.safe_load(f)\n",
    "    experiment_names = list(experiments.keys())\n",
    "    experiment_names = experiment_names[1:7]  # Select the first 6 experiment names\n",
    "    #experiment_names = ['01_baseline',\n",
    "    #                    '02_pixel_composites',\n",
    "    #                    '03_seasonal_composites',\n",
    "    #                    '04_pixel_and_seasonal_comp',\n",
    "    #                    '05_support_fmask_dlt',\n",
    "    #                      '06_support_fmask_dlt_dem',\n",
    "                        #'07_aux_task',\n",
    "                       # ]\n",
    "    \n",
    "with open('../configs/normparams.yaml', 'r') as f:\n",
    "    normparams = yaml.safe_load(f)\n",
    "joint_normparams = normparams['chm']['_111']\n",
    "#print(experiment_names)\n",
    "experiment_names\n",
    "sites, cfg = cf.get_config('01_baseline')  \n",
    "combos = [\"110\",\"101\",\"011\"] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "408cfe3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['02_pixel_composites',\n",
       " '03_seasonal_composites',\n",
       " '04_pixel_and_seasonal_comp',\n",
       " '05_support_fmask_dlt',\n",
       " '06_support_fmask_dlt_dem',\n",
       " '07_aux_task']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fca4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "global_config = md.global_config\n",
    "# reproducible shuffling (num_workers=0 -> simpler)\n",
    "seed = global_config['seed']\n",
    "run_id_base = \"251021_GEN_\"\n",
    "repetitions = 2\n",
    "combos = [\"110\",\"101\",\"011\"] #1 is training data, 0 test. logic is LSB: 001 -> SITE1 is training data. \n",
    "\n",
    "for i in range(repetitions): # Run 10 experiments with different seeds\n",
    "\n",
    "    for combo in combos:\n",
    "        #run_id = md.generate_run_id()\n",
    "        run_id = run_id_base + f\"_{combo}_{i}\"\n",
    "        seed = seed + i  # Different seed for each experiment\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "        for exp_name in experiment_names:\n",
    "            \n",
    "            sites, cfg = cf.get_config(exp_name)  \n",
    "            cfg.update(global_config)  # Ensure cfg has the latest global_config\n",
    "            cfg.update({\"combo\": combo})\n",
    "            print(\"=== NEW EXPERIMENT ===\")\n",
    "            print(f\" --> Name: {exp_name}, Combo: {combo}, Run ID: {run_id}, Seed: {seed}\")\n",
    "\n",
    "            # Build dataset\n",
    "            #X, Y = dt.build_patched_dataset(cfg, sites, patch_size=32, nan_percent_allowed=20)\n",
    "            X_patch_train, Y_patch_train, X_test, Y_test = dt.build_patched_dataset_generalization(cfg, sites, combo,patch_size=32, nan_percent_allowed=20)\n",
    "            \n",
    "            # now split the train data into train (80%) and val (20%)\n",
    "            X_train, X_val, Y_train, Y_val = train_test_split(\n",
    "                X_patch_train, Y_patch_train, test_size=0.2, random_state=seed\n",
    "            )\n",
    "\n",
    "            # split into train/val/test (70/15/15) with your logic\n",
    "            #(X_train, Y_train), (X_val, Y_val), (X_test, Y_test) = dt.split_dataset(X, Y, seed=seed)\n",
    "\n",
    "            # Syntax: X_train, X_val, X_test, y_train, y_val, y_test \n",
    "            train_dataset = md.S2CanopyHeightDataset(X_train, Y_train, cfg)\n",
    "            val_dataset = md.S2CanopyHeightDataset(X_val, Y_val, cfg)\n",
    "            test_dataset = md.S2CanopyHeightDataset(X_test, Y_test, cfg)\n",
    "\n",
    "            train_loader = md.DataLoader(\n",
    "                train_dataset,\n",
    "                batch_size=global_config['batch_size'],\n",
    "                shuffle=True,\n",
    "                num_workers=0\n",
    "            )\n",
    "            val_loader = md.DataLoader(val_dataset, batch_size=global_config['batch_size'])\n",
    "            test_loader = md.DataLoader(test_dataset, batch_size=global_config['batch_size'])\n",
    "            \n",
    "            if False:\n",
    "                print(f\"Train dataset valid pixel share: \\t{train_dataset.validshare():.4f}, count: {len(train_dataset)}\")\n",
    "                print(f\"Validation dataset valid pixel share: \\t{val_dataset.validshare():.4f}, count: {len(val_dataset)}\")\n",
    "                print(f\"Test dataset valid pixel share: \\t{test_dataset.validshare():.4f}, count: {len(test_dataset)}\")\n",
    "            \n",
    "            # build model depending on in out channels, defined by the dataloaders\n",
    "            model = md.build_unet(in_channels=X_train[0].shape[0], out_channels=Y_train[0].shape[0])\n",
    "            #train model depending on config. \n",
    "            model, logs = md.train_model(model, train_loader, val_loader, cfg, md.global_config)\n",
    "\n",
    "            # evaluate model on val and test set, save results\n",
    "            # get normparams for CHM\n",
    "            combo_key = f\"_{combo}\"\n",
    "            normparams_used = normparams['chm'][combo_key]\n",
    "\n",
    "            md.save_results(model, val_loader, test_loader, normparams_used, logs, cfg, run_id=run_id)\n",
    "\n",
    "            print(\"=================\")\n",
    "\n",
    "        print(\"DONE WITH COMBO \", combo)\n",
    "    print(\"DONE WITH ALL EXPERIMENTS for iteration:  \", i)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "786e1b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "combo_key = f\"_{110}\"\n",
    "\n",
    "normparams_used = normparams['chm'][combo_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "698a0a4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mu': 28.23365592956543, 'n': 755154, 'std': 5.794953346252441}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normparams_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e74b6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix the preds and save them again\n",
    "\n",
    "global_config = md.global_config\n",
    "# reproducible shuffling (num_workers=0 -> simpler)\n",
    "seed = global_config['seed']\n",
    "run_id_base = \"251021_GEN_\"\n",
    "repetitions = 1 # only the first rep was done so far\n",
    "combos = [\"110\",\"101\",\"011\"] #1 is training data, 0 test. logic is LSB: 001 -> SITE1 is training data. \n",
    "\n",
    "for i in range(repetitions): # Run 10 experiments with different seeds\n",
    "\n",
    "    for combo in combos:\n",
    "        #run_id = md.generate_run_id()\n",
    "        run_id = run_id_base + f\"_{combo}_{i}\"\n",
    "        seed = seed + i  # Different seed for each experiment\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "        for exp_name in experiment_names:\n",
    "            \n",
    "            sites, cfg = cf.get_config(exp_name)  \n",
    "            cfg.update(global_config)  # Ensure cfg has the latest global_config\n",
    "            cfg.update({\"combo\": combo})\n",
    "            print(\"=== NEW EXPERIMENT ===\")\n",
    "            print(f\" --> Name: {exp_name}, Combo: {combo}, Run ID: {run_id}, Seed: {seed}\")\n",
    "\n",
    "            # Build dataset\n",
    "            #X, Y = dt.build_patched_dataset(cfg, sites, patch_size=32, nan_percent_allowed=20)\n",
    "            X_patch_train, Y_patch_train, X_test, Y_test = dt.build_patched_dataset_generalization(cfg, sites, combo,patch_size=32, nan_percent_allowed=20)\n",
    "            \n",
    "            # now split the train data into train (80%) and val (20%)\n",
    "            X_train, X_val, Y_train, Y_val = train_test_split(\n",
    "                X_patch_train, Y_patch_train, test_size=0.2, random_state=seed\n",
    "            )\n",
    "\n",
    "            # split into train/val/test (70/15/15) with your logic\n",
    "            #(X_train, Y_train), (X_val, Y_val), (X_test, Y_test) = dt.split_dataset(X, Y, seed=seed)\n",
    "\n",
    "            # Syntax: X_train, X_val, X_test, y_train, y_val, y_test \n",
    "            train_dataset = md.S2CanopyHeightDataset(X_train, Y_train, cfg)\n",
    "            val_dataset = md.S2CanopyHeightDataset(X_val, Y_val, cfg)\n",
    "            test_dataset = md.S2CanopyHeightDataset(X_test, Y_test, cfg)\n",
    "\n",
    "            train_loader = md.DataLoader(\n",
    "                train_dataset,\n",
    "                batch_size=global_config['batch_size'],\n",
    "                shuffle=True,\n",
    "                num_workers=0\n",
    "            )\n",
    "            val_loader = md.DataLoader(val_dataset, batch_size=global_config['batch_size'])\n",
    "            test_loader = md.DataLoader(test_dataset, batch_size=global_config['batch_size'])\n",
    "            \n",
    "            if False:\n",
    "                print(f\"Train dataset valid pixel share: \\t{train_dataset.validshare():.4f}, count: {len(train_dataset)}\")\n",
    "                print(f\"Validation dataset valid pixel share: \\t{val_dataset.validshare():.4f}, count: {len(val_dataset)}\")\n",
    "                print(f\"Test dataset valid pixel share: \\t{test_dataset.validshare():.4f}, count: {len(test_dataset)}\")\n",
    "            \n",
    "            # build model depending on in out channels, defined by the dataloaders\n",
    "            model = md.build_unet(in_channels=X_train[0].shape[0], out_channels=Y_train[0].shape[0])\n",
    "            #train model depending on config. \n",
    "            #model, logs = md.train_model(model, train_loader, val_loader, cfg, md.global_config)\n",
    "\n",
    "            model_weights, logs, cfg = md.load_results(exp_name, run_id)\n",
    "            model.load_state_dict(model_weights) # fix the preds and save them again\n",
    "            # evaluate model on val and test set, save results\n",
    "            # get normparams for CHM\n",
    "            combo_key = f\"_{combo}\"\n",
    "            normparams_used = normparams['chm'][combo_key]\n",
    "\n",
    "            md.save_results(model, val_loader, test_loader, normparams_used, logs, cfg, run_id=run_id)\n",
    "\n",
    "            print(\"=================\")\n",
    "\n",
    "        print(\"DONE WITH COMBO \", combo)\n",
    "    print(\"DONE WITH ALL EXPERIMENTS for iteration:  \", i)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CHM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
